# Here we provide a default model architecture that balances accuracy, speed, and 
# memory consumption. In general, TACE provides two alternative architecture configurations.

# 1. Using 2-layer architecture combined with correlation 3. 

# 2. Using 3–5 layer architecture with correlation 2, together with the constraint l1 ≤ l2.

# In most cases, the first option is sufficient. While the second option may offer 
# advantages for more complex datasets. 

# The inclusion of nonlinear modules in the interaction term should be handled 
# with caution and is typically disabled by default.

# Note that all results reported in our original paper were obtained using
# 2-layer ACE configuration. https://arxiv.org/abs/2509.14961

config:
  _target_: tace.models.TACEV1 # only TACEV1 are avaliable now
  wrapper:
    _target_: tace.models.WrapModelV1 # only WrapModelV1 are avaliable now

  cutoff: 4.23 # (float), recommend 6.0
  max_neighbors: null # (null, int) if you want to use tace in lammps, do not use max_neighbors
  atomic_numbers: null # (null, list), null = read from dataset.
  atomic_energies: null
  # atomic_energies: # (list[dict[int, float]], null) null = auto caculate, for each computational level
  #   - {1: -13.587222780835477, 6: -1029.4889999855063, 7: -1484.9814568572233, 8: -2041.9816003861047}
  num_levels: 1
  use_multi_head: false # (bool)
  use_multi_fidelity: false # (bool)
  Lmax: 2 # (int, list), Truncation for node, 2 is recommended. If >2, recommend set `l1l2 = <=`
  lmax: 3 # (int, list), Truncation for edge, 3 is recommended, 2 for higher speed. If >3, recommend set `l1l2 = <=`
  num_layers: 2 # (int), 2 for correlation >=2, can be higher when correlation=2
  bias: true # (bool), false is more safe, but true sometimes give better results
  num_channel: 48 # (int), recommended 48-64, 64's accuracy is enough to achieve SOTA
  num_channel_hidden: ${model.config.num_channel} # (int), equal num_channel is the best
  
  radial_basis:
    radial_basis: j0   # [j0, jn, n_j0]
    num_radial_basis: 8 # (int) 8-10 is recommended 
    distance_transform: null # (null, str) choices: [null, Agnesi, Soft], this plug-in from mace, if you do not know what it means, never use
    polynomial_cutoff: 5 # (int, float)
    order: 0 # (int), specify order for jn, for j0, it is negligible, for jn, you should use float64
    trainable: false # (bool)
    apply_cutoff: true # # (bool) true: cutoff was use in before radial_mlp, false: after radial_mlp

  angular_basis:
    traceless: true # (bool), always set this to true
    norm: true      # (bool), always set this to true

  radial_mlp:
    hidden: [64, 64, 64] # (list(int), list(list(int)))
    act: 'silu'   # (str, null)
    bias: false   # (bool)

  inter:
    l1l2: null # (null | str | list), choices: [null, <=], restriction for each layer
    conv_weights: [edge_ij] # list, subset of [edge_ij, node_i, node_j], at least [edge_ij]
    normalizer: avg_num_neighbors  # str, choices: [avg_num_neighbors, density_v1]
    nonlinearity:
      type: null # (null | str), choices: [null, norm, gated]
      gate: 'silu' # (null | str)
    sc: # self connection
      use_first_sc: false    # (bool), false is recommended
    use_resnet: false        # (bool), recommend true when use nonlinearity gate, else false

  prod:
    l1l2: null  # (null | str | list), restriction when produce combination for each layer, choices: [null, <=]
    l3l1: null  # (null | str | list), restriction when produce combination for each layer, choices: [null, <=, ==]
    correlation: 3 # (int | list) body-order, 3 is recommended
    element_aware: true # (bool) always set this to true
    coupled_channel: true # (bool) always set this to true
    precompute: false # only allowed correlation = 2, 3 is to large, l1l2, l3l1 is useless when this is True
 
  readout_emlp:
    hidden: [16] # (list(int)) always set this to [16]
    act: silu # (null | str) for l=0, always set this to silu
    gate: silu # (null | str) for l>0
    bias: false # (bool) only useful for l=0, recommended false
    use_all_layer: true # (bool) recommended true, if true, evergy layer has readout, else, only last layer has
    enable_uie_readout: false # (bool), whether use universal invariant embedding readout
    
  scale_shift:
    scale_type: rms_forces_by_element # (null | str) add by_element can scale for each element
    shift_type: null # (null | str)  add by_element can shift for each element,  for energy only, set ``null`` or ``std_energy``
    scale_trainable: false # (bool) 
    shift_trainable: false # (bool) 

  short_range:
    zbl: # metal units, for molecular dynamics, true is recommended
      enable: true
      trainable: false # recommended always false

  long_range:
    les: # for introduction to the arguments, see the official repo: https://github.com/ChengUCB/les
      enable: false
      les_arguments:
        n_layers: 3
        n_hidden: [32, 16]
        add_linear_nn: true
        output_scaling_factor: 0.1
        sigma: 1.0
        dl: 2.0
        remove_self_interaction: true
        remove_mean: true
        epsilon_factor: 1.0
        use_atomwise: false
        compute_bec: false
        bec_output_index: null

  conservation: # only one of enable_* can be true for each property
    charges:
      method: lagrangian # (str) choices: [lagrangian, uniform_distribution] 

  universal_embedding:
    invariant:
      level:
        enable: false
        num_embeddings: 3 # (int) the number of different DFT computational levels
      
      spin_on:
        enable: false
        num_embeddings: 2 # (int) 

      spin_multiplicity:
        enable: false
        num_embeddings: 3 # (int) 
      
      charges:
        enable: false
        act: 'silu'   # (str, null)

      total_charge:
        enable: false
        act: 'silu'   # (str, null)

      initial_collinear_magmoms:
        enable: false
        act: 'silu'   # (str, null)
    
      temperature:
        enable: false
        act: 'silu'   # (str, null)

      electron_temperature:
        enable: false
        act: 'silu'   # (str, null)
   
    equivariant:
      electric_field:
        enable: false
        normalizer: 1.0 # (float)
        gate: null      # (str, null)
        linear: False   # (bool)

      magnetic_field:
        enable: false
        normalizer: 1.0 # (float)
        gate: null      # (str, null)
        linear: False   # (bool)

      initial_noncollinear_magmoms:
        enable: false
        normalizer: 1.0 # (float)
        gate: null      # (str, null)
        linear: False   # (bool)

  special: # this field can be thoroughly ignored if you only interested in E/F/S training
    hessians:
      num_samples: 2 # traing E/F/Hessian, for each graph, <= num_samples atoms's hessians will be used to train



 

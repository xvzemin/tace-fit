defaults:
  - _self_ 
  # Specify the logger backlend
  # - config/logger@_here_: wandb
  # Specify the model architecture
  - config/model@model: tacev1.yaml

# Specify the path to resume training, Files must be end with .ckpt
resume_from_model: null

# Specify the model to be fine-tuned. Files must be end with .ckpt, .pt, or .pth
finetune_from_model: null 

finetune: null # this field equals ./finetune_config.yaml, auto read or write directly here

misc:
  project_name: 3bpa
  global_seed: 42 
  device: cuda # cpu or cuda
  allow_tf32: false
  ignore_warning: true 
  log_level: info
  env: # You can specify here the environment variables that should always be used.
    WANDB_MODE: offline # For the convenience of Chinese users

trainer:
  # As long as your system has NVIDIA GPUs, the following configuration will automatically detect and use all available GPUs for training.
  # If you wish to control which GPUs are visible, you can do so by setting the environment variable like export CUDA_VISIBLE_DEVICES=0,2. 
  _target_: lightning.Trainer
  num_nodes: 1   
  accelerator: auto
  devices: auto        # auto or list
  max_time: '90:00:00:00' # 90 days
  max_epochs: 20000
  min_epochs: 1        # null or int > 1
  precision: 32        # 64 or 32
  strategy: auto # For single-GPU training, use auto
  # strategy: # Recommended for multi-GPU training. 
  #   _target_: tace.utils.strategy.SimpleDDPStrategy                                                  
  gradient_clip_val: 10.0 
  enable_progress_bar: true # If you don't want to see the progress bar, you can turn it off
  log_every_n_steps: 50

  # Generally, no modification is required
  enable_model_summary: true
  enable_checkpointing: true 
  check_val_every_n_epoch: 1
  detect_anomaly: false
  inference_mode: false
  deterministic: false  
  # benchmark: true  

callbacks:
  early_stopping:
    _target_: lightning.pytorch.callbacks.EarlyStopping
    verbose: true
    log_rank_zero_only: true
    monitor: ${synth_metric.monitor_metric_name}
    # min_delta: 1e-5 
    patience: 100

  ema: # ema is always recommended
    _target_: tace.utils.callbacks.EMACallback
    decay: 0.99 # 0.99 - 0.999
    use_num_updates: true

  checkpoint: # at leas one checkpoint and key=checkpoint is required
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: checkpoints
    filename: TACE-3BPA-{epoch}-{step}-{${synth_metric.monitor_metric_name}:.4f}
    monitor: ${synth_metric.monitor_metric_name}
    save_top_k: 3
    save_last: true
    # every_n_epochs: 1
    # every_n_train_steps: 2000
    # mode: min
    save_weights_only: false
    auto_insert_metric_name: false
    verbose: true

  # checkpoint_step:
  #   _target_: lightning.pytorch.callbacks.ModelCheckpoint
  #   dirpath: checkpoints
  #   filename: TACE-OMat24-{epoch}-{step}-{${synth_metric.monitor_metric_name}:.4f}
  #   # monitor: val/loss
  #   save_top_k: -1
  #   # every_n_epochs: 1
  #   every_n_train_steps: 2000
  #   mode: min
  #   save_weights_only: false
  #   auto_insert_metric_name: false
  

dataset:
  type: ase # ase or ase-db
  split_seed: ${misc.global_seed} # this random seed is useful if auto split
  train_file: dataset/train_300K.xyz
  valid_file: null                
  # test_files: # (null, str, list) recoomend to use tace-eval --test 1 to print test errors to avoild possible bugs
  #   - dataset/test_300K.xyz
  #   - dataset/test_600K.xyz
  #   - dataset/test_1200K.xyz
  #   - dataset/test_dih.xyz
  valid_ratio: 0.1 # auto split from train file if valid file is null, The priority order is:  no_valid_set > valid_file > valid_from_index > ``valid_ratio.
  valid_from_index: false # split train and val from train.index and valid.index in current directory
  no_valid_set: false # The prerequisite for enabling this is that you are using a learning rate scheduler that does not depend on the validation set.
  neighborlist_backend: matscipy # [ase, vesin, matscipy] recommend matscipy
  storage_mode: memory # memory or lmdb, if your dataset is large (> 100w), the recommended approach is to use lmdb, as this avoids repeatedly constructing the graph.  
  shard_dirs: # if lmdb model, specify a list of path where you save you graph
    - graphCache
  # If using LMDB mode, you must allocate a reasonable pre-storage size.  
  # Generally, if the number of neighboring atoms is around 60, about 200 KB per graph is sufficient.
  shard_size: 10000 # number of graphs stored per file in LMDB mode, should be large, here is just an example, if small, it will be slow and may be error
  cache_size: 1024 # cache for faster load data from dataloader
  avg_graph_size_in_KB: 200 # in KB, the total disk usage equals the size of this multiplied by the total number of your structures.
  lmdb_wait_timeout: 86400 # in seconds; when training with multi-GPU, the maximum waiting time.
  force_dtype: null # 32 or 64, if your data grapg saved in lmdb, but original float64, you can force to float32 for convenience
  keys: # all default key name is the property name
    level_key: level
    energy_key: energy
    forces_key: forces
    stress_key: stress
    virials_key: virials
    hessians_key: hessians
    charges_key: charges
    total_charge_key: total_charge
    spin_multiplicity_key: spin_multiplicity
    temperature_key: temperature
    electron_temperature_key: electron_temperature
    direct_forces_key: forces
    direct_stress_key: stress
    direct_virials_key: virials
    polarization_key: polarization
    direct_dipole_key: dipole
    conservative_dipole_key: dipole
    direct_polarizability_key: polarizability
    conservative_polarizability_key: polarizability
    born_effective_charges_key: born_effective_charges
    electric_field_key: electric_field
    magnetic_field_key: magnetic_field
    initial_collinear_magmoms_key: initial_collinear_magmoms
    initial_noncollinear_magmoms_key: initial_noncollinear_magmoms
    final_collinear_magmoms_key: final_collinear_magmoms
    final_noncollinear_magmoms_key: final_noncollinear_magmoms
    total_collinear_magmoms_key: total_collinear_magmoms
    total_noncollinear_magmoms_key: total_noncollinear_magmoms
    collinear_magnetic_forces_key: collinear_magnetic_forces
    noncollinear_magnetic_forces_key: noncollinear_magnetic_forces
    spin_on_key: spin_on


  train_dataloader:
    _target_: torch_geometric.loader.DataLoader
    batch_size: 5
    shuffle: true # always true
    drop_last: false
    num_workers: 0 # If the batch size is large, you need to set up multiple workers

  valid_dataloader:
    _target_: torch_geometric.loader.DataLoader
    batch_size: 25
    shuffle: false
    drop_last: false
    num_workers: 0 # If the batch size is large, you need to set up multiple workers

  test_dataloader: ${dataset.valid_dataloader}

# optimizer:
#   _target_: torch.optim.Adam
#   lr: 1e-3
#   weight_decay: 0.0
#   amsgrad: true

optimizer:
 _target_: torch.optim.Adam
 lr: 1e-2
 weight_decay: 0.0 # 1e-3, 1e-8
 amsgrad: true

scheduler: # validaton-based
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  mode: min
  factor: 0.8
  # min_lr: 1e-6
  patience: 50
  # threshold: 1e-4 
  extra:
    monitor: ${synth_metric.monitor_metric_name}
    interval: epoch
    frequency: 1

# # support resolver like add, sub, mul, float, ceil, floor
# scheduler: # fixed-step
#   _target_: tace.utils.lr_scheduler.CosineAnnealingWarmupRestarts
#   first_cycle_steps: 400000 # total step, one batch = one step
#   cycle_mult: 1.0 # restart factor
#   max_lr: 2e-4 
#   min_lr: 2e-6
#   warmup_steps: ${floor:${mul:${scheduler.first_cycle_steps}, 0.05}} # 5 % first step (total here)
#   gamma: 1.0 # decay factor
#   last_epoch: -1
#   extra:
#     interval: step
#     frequency: 1

synth_metric: 
  monitor_metric_name: val/forces_rmse # use for variable interpolation

# # recommended for actual production and use
# synth_metric: 
#   # All metrics can be accessed by combining the stage (train | val) with the property name and the metric type (mae or rmse), for example:
#   # val/energy_mae, val/energy_per_atom_rmse, val/forces_mae, val/stress_rmse, val/loss, train/loss ...
#   # Specifically, we have a composite metric, val/synth_metric, for which we can specify a mixing ratio. This is particularly useful when training on large datasets with outliers.
#   monitor_metric_name: val/synth_metric # use for variable interpolation
#   # MAE
#   val/energy_mae: 1.0
#   val/energy_per_atom_mae: 5.0
#   val/forces_mae: 5.0
#   val/stress_mae: 2.5
#   # RMSE
#   val/energy_per_atom_rmse: 1.0
#   val/forces_rmse: 1.0
#   val/stress_rmse: 1.0

loss:
  _target_: tace.utils.loss.NormalLoss
  loss_property: [energy, forces] # [energy, forces, stress]
  loss_function_name: 
    - mse_energy_per_atom
    - mse_forces
    # - mse_stress
  loss_property_weights: [1.0, 10.0] # [1.0, 5.0, 0.1]
  loss_huber_delta: 0.01

# The performance is mediocre and cannot achieve full convergence, but it can be used as a toy loss.
# loss:
#   _target_: tace.utils.loss.UncertaintyLoss
#   loss_property: [energy, forces]
#   loss_function_name: 
#     - mse_energy_per_atom
#     - mse_forces
#     # - mse_stress
#   loss_property_weights: [1.0, 1.0]
#   loss_huber_delta: [0.01, 0.01]








defaults:
  - _self_ 
  # Specify the logger backlend
  - config/logger@_here_: wandb
  # Specify the model architecture
  - config/model@model: cart_tacev1 

# Specify the path to resume training, Files must be end with .ckpt
resume_from_model: null

# Specify the model to be fine-tuned. Files must be end with .ckpt, .pt, or .pth
finetune_from_model: null

finetune: null # this field equals ./finetune_config.yaml, auto read or write directly here

misc:
  project_name: C10H2+C10H3
  global_seed: 42 
  device: cuda # cpu or cuda
  allow_tf32: true
  ignore_warning: true 
  env: # You can specify here the environment variables that should always be used.
    WANDB_MODE: offline # For the convenience of Chinese users
  log_level: info       # debug or info

trainer:
  # As long as your system has NVIDIA GPUs, the following configuration will automatically detect and use all available GPUs for training.
  # If you wish to control which GPUs are visible, you can do so by setting the environment variable like export CUDA_VISIBLE_DEVICES=0,2. 
  _target_: lightning.Trainer
  num_nodes: 1   
  accelerator: auto
  devices: auto        # auto or list
  max_time: '90:00:00:00' # 90 days
  max_epochs: 20000
  min_epochs: 1        # null or int > 1
  precision: 32        # 64 or 32
  strategy: auto # For single-GPU training, use auto
  # strategy: # Recommended for multi-GPU training. 
  #   _target_: tace.utils.strategy.SimpleDDPStrategy                                                  
  gradient_clip_val: 1.0
  enable_progress_bar: false # If you don't want to see the progress bar, you can turn it off
  log_every_n_steps: 100

  # Generally, no modification is required
  enable_model_summary: true
  enable_checkpointing: true 
  check_val_every_n_epoch: 1
  detect_anomaly: false
  inference_mode: false
  deterministic: false  
  # benchmark: true  

callbacks:
  early_stopping:
    _target_: lightning.pytorch.callbacks.EarlyStopping
    verbose: true
    log_rank_zero_only: true
    monitor: ${synth_metric.monitor_metric_name}
    # min_delta: 1e-5 
    patience: 25

  ema: # ema is always recommended
    _target_: tace.utils.callbacks.EMACallback
    decay: 0.99 # 0.99 - 0.999
    use_num_updates: true

  checkpoint: # at leas one checkpoint and key=checkpoint is required
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: checkpoints
    filename: TACE-${misc.project_name}-{epoch}-{step}-{${synth_metric.monitor_metric_name}:.4f}
    monitor: ${synth_metric.monitor_metric_name}
    save_top_k: 10
    save_last: true
    # every_n_epochs: 1
    # every_n_train_steps: 2000
    # mode: min
    save_weights_only: false
    auto_insert_metric_name: false
    verbose: true

dataset:
  type: ase # ase or ase-db
  split_seed: ${misc.global_seed} # this random seed is useful if auto split
  train_file: data/C10H2+C10H3.xyz
  valid_file: null                
  # test_files: # (null, str, list) recoomend to use tace-eval --test 1 to print test errors to avoild possible bugs
  #   - dataset/test_300K.xyz
  #   - dataset/test_600K.xyz
  #   - dataset/test_1200K.xyz
  #   - dataset/test_dih.xyz
  valid_ratio: 0.1 # auto split from train file if valid file is null, The priority order is:  no_valid_set > valid_file > valid_from_index > ``valid_ratio.
  valid_from_index: false # split train and val from train.index and valid.index in current directory
  no_valid_set: false # The prerequisite for enabling this is that you are using a learning rate scheduler that does not depend on the validation set.
  neighborlist_backend: matscipy # [ase, vesin, matscipy] recommend matscipy
  storage_mode: memory # memory or lmdb, if your dataset is large (> 100w), the recommended approach is to use lmdb, as this avoids repeatedly constructing the graph.  
  shard_dirs: # if lmdb model, specify a list of path where you save you graph
    - graphCache
  # If using LMDB mode, you must allocate a reasonable pre-storage size.  
  # Generally, if the number of neighboring atoms is around 60, about 200 KB per graph is sufficient.
  shard_size: 50000 # number of graphs stored per file in LMDB mode, should be large, here is just an example, if small, it will be slow and may be error
  cache_size: 1024 # cache for faster load data from dataloader
  avg_graph_size_in_KB: 500 # 200 # in KB, the total disk usage equals the size of this multiplied by the total number of your structures.
  lmdb_wait_timeout: 86400 # in seconds; when training with multi-GPU, the maximum waiting time.
  force_dtype: null # 32 or 64, if your data grapg saved in lmdb, but original float64, you can force to float32 for convenience
  keys: # all default key name is the property name
    level_key: level
    energy_key: energy
    forces_key: forces
    stress_key: stress
    virials_key: virials
    hessians_key: hessians
    charges_key: initial_charges
    total_charge_key: total_charge
    spin_multiplicity_key: spin_multiplicity
    temperature_key: temperature
    electron_temperature_key: electron_temperature
    direct_forces_key: forces
    direct_stress_key: stress
    direct_virials_key: virials
    polarization_key: polarization
    direct_dipole_key: dipole
    conservative_dipole_key: dipole
    direct_polarizability_key: polarizability
    conservative_polarizability_key: polarizability
    born_effective_charges_key: born_effective_charges
    electric_field_key: electric_field
    magnetic_field_key: magnetic_field
    initial_collinear_magmoms_key: initial_collinear_magmoms
    initial_noncollinear_magmoms_key: initial_noncollinear_magmoms
    final_collinear_magmoms_key: final_collinear_magmoms
    final_noncollinear_magmoms_key: final_noncollinear_magmoms
    total_collinear_magmoms_key: total_collinear_magmoms
    total_noncollinear_magmoms_key: total_noncollinear_magmoms
    collinear_magnetic_forces_key: collinear_magnetic_forces
    noncollinear_magnetic_forces_key: noncollinear_magnetic_forces
    spin_on_key: spin_on

  train_dataloader:
    _target_: torch_geometric.loader.DataLoader
    batch_size: 16 # larger is better, It is recommended to utilize around 80% of your GPU memory for safety.
    shuffle: true # always true
    drop_last: false
    num_workers: 0 # If the batch size is large, you need to set up multiple workers

  valid_dataloader:
    _target_: torch_geometric.loader.DataLoader
    batch_size: ${dataset.train_dataloader.batch_size}
    shuffle: false
    drop_last: false
    num_workers: ${dataset.train_dataloader.num_workers}

  test_dataloader: ${dataset.valid_dataloader}

optimizer:
  _target_: torch.optim.Adam
  lr: 1e-3
  weight_decay: 0.0
  amsgrad: true
  betas: [0.9, 0.999]

  # _target_: torch.optim.AdamW
  # lr: 4e-3
  # weight_decay: 1e-8  
  # amsgrad: false

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau 
  mode: "min"
  factor: 0.5
  #min_lr: 0.00001 
  patience: 100
  extra:
    monitor: val/loss
    interval: epoch
    frequency: 1

# # support resolver like add, sub, mul, float, ceil, floor
# scheduler: # fixed-step
#   _target_: tace.utils.lr_scheduler.CosineAnnealingWarmupRestarts
#   first_cycle_steps: 400000 # total step, one batch = one step
#   cycle_mult: 1.0 # restart factor
#   max_lr: 4e-3
#   min_lr: 5e-4
#   warmup_steps: ${floor:${mul:${scheduler.first_cycle_steps}, 0.05}} # 5 % first step (total here)
#   gamma: 1.0 # decay factor
#   last_epoch: -1
#   extra:
#     interval: step
#     frequency: 1

# recommended for actual production and use
synth_metric: 
  # All metrics can be accessed by combining the stage (train | val) with the property name and the metric type (mae or rmse), for example:
  # val/energy_mae, val/energy_per_atom_rmse, val/forces_mae, val/stress_rmse, val/loss, train/loss ...
  # Specifically, we have a composite metric, val/synth_metric, for which we can specify a mixing ratio. 
  # This is particularly useful when training on large datasets with outliers.
  monitor_metric_name: val/loss # used for variable interpolation
  # # MAE
  # val/energy_mae: 1.0
  # val/energy_per_atom_mae: 5.0
  # val/forces_mae: 5.0
  # val/stress_mae: 2.5
  # # RMSE
  # val/energy_per_atom_rmse: 1.0
  # val/forces_rmse: 1.0
  # val/stress_rmse: 1.0

# Huber loss, recommended for actual production and use
# To obtain the best possible model performance, we recommend a two-stage training strategy.
# In the first stage, use the recommended loss weights of 1:8:8.
# In the second stage, retrain the model using loss weights of 1:1:2,
# and apply SWA (tace-average commmand) to average the model parameters.
# Note that EMA is not used in the second stage.
# If this two-stage procedure is too cumbersome, an alternative is to perform
# a single training run with reduced weights for forces and stress.
# loss:
#   _target_: tace.utils.loss.OMat24sAlexMPtrjLoss
#   loss_property: [energy, forces, stress]
#   energy_weight: 1.0
#   forces_weight: 8.0
#   stress_weight: 8.0
#   energy_huber_delta: 0.01
#   forces_huber_delta: 0.01
#   stress_huber_delta: 0.01

loss:
  _target_: tace.utils.loss.NormalLoss
  loss_property: [energy, forces, charges]
  loss_function_name:
    - mse_energy_per_atom
    - mse_forces
    - mse_charges
  loss_property_weights: [10.0, 10.0, 1.0]

# loss:
#   _target_: tace.utils.loss.NormalLoss
#   loss_property: [energy, forces, stress, polarization, conservative_polarizability, born_effective_charges] 
#   loss_function_name: 
#     - mse_energy_per_atom
#     - mse_forces
#     - mse_stress
#     - mse_polarization_per_atom
#     - mse_conservative_polarizability_per_atom
#     - mse_born_effective_charges
#   loss_property_weights: [1, 1, 1, 1, 1000, 1] 

# # The performance is mediocre and cannot achieve full convergence, but it can be used as a toy loss.
# loss:
#   _target_: tace.utils.loss.NormalLoss
#   loss_property: [energy, forces, stress, polarization, conservative_polarizability, bonn_effective_charges] 
#   loss_function_name: 
#     - mse_energy_per_atom
#     - mse_forces
#     - mse_stress
#     - mse_polarization_per_atom
#     - mse_conservative_polarizability_per_atom
#     - bonn_effective_charges
#   loss_property_weights: [1, 1, 1, 1, 1000, 1] 








